{
  "device": "cpu",
  "device_idx": "1",
  "llm_name": "llama-3.2-1b",
  "model_name_or_path": "meta-llama/Llama-3.2-1B",
  "tokenizer_name_or_path": "meta-llama/Llama-3.2-1B",
  "peft_type": "CAUSAL_LM",
  "task_type": "TEXT",
  "num_virtual_tokens": 50,
  "prompt_tuning_init_text": "please generate query for this document",
  "peft_model_id": "llama-3.2-1b_CAUSAL_LM_TEXT",
  "prompt_num": 3,
  "text_len": 1024,
  "dataset_name": "law",
  "train_data": "xuyang/data/law/prompt_tuning_train_text.csv",
  "eval_data": "xuyang/data/law/prompt_tuning_test_text.csv",
  "test_data": "xuyang/data/law/prompt_tuning_test_text.csv",
  "few_shot_num": 50,
  "fixed_prompt": true,
  "max_length": 1024,
  "lr": 0.03,
  "num_epochs": 2,
  "batch_size": 1,
  "eval_batch_size": 2,
  "checkpoint_name": "law_meta-llama_Llama-3.2-1B_CAUSAL_LM_TEXT_v1.pt",
  "experiment_dir": "llm_models",
  "experiment_description": "v1_pointwise_without_prompt_example_law_llama-3.2-1b_llama-3.2-1b_CAUSAL_LM_TEXT_50_50_3_fixed_prompt_contractive_hard_10_val_loss"
}
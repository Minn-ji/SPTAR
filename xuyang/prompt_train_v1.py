from transformers import AutoModelForCausalLM
from peft import get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType
import os
import wandb
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
from transformers import default_data_collator, get_linear_schedule_with_warmup
from tqdm import tqdm
from pathlib import Path
import argparse
from dotenv import load_dotenv
from xuyang.args import PromptTuringArgs
from xuyang.utils import reset_args, AverageMeter, setup_train
from xuyang.dataset import MSMARCODataset

load_dotenv()

hugging_face_api_key = os.getenv("HUGGINGFACE_API_KEY")

import torch

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

torch.cuda.empty_cache()
torch.cuda.ipc_collect()
def _sanitize_and_check(batch, model, tokenizer):
    # dtype Î≥¥Ï†ï
    if batch["labels"].dtype != torch.long:
        batch["labels"] = batch["labels"].long()

    # ÎùºÎ≤® Î≥¥Ï†ï: ÏùåÏàò Ï†ÑÎ∂Ä -100Î°ú, vocab_size Ïù¥ÏÉÅÎèÑ -100
    with torch.no_grad():
        labels = batch["labels"]
        labels[labels < 0] = -100
        labels[labels >= model.config.vocab_size] = -100
        batch["labels"] = labels

    # input_ids Î≤îÏúÑ Í≤ÄÏÇ¨ (ÏûÑÎ≤†Îî© Ïù∏Îç±Ïä§ Ïò§Î•ò Î∞©ÏßÄ)
    ids = batch["input_ids"]
    V = model.config.vocab_size
    if not torch.all((ids >= 0) & (ids < V)):
        # Ïñ¥ÎîîÏÑú ÌÑ∞Ï°åÎäîÏßÄ Î∞îÎ°ú Ïïå Ïàò ÏûàÍ≤å ÏòàÏô∏
        mn = int(ids.min().item()); mx = int(ids.max().item())
        raise RuntimeError(f"input_ids out of range: min={mn}, max={mx}, vocab_size={V}")

    return batch

def load_tokenizer(args):
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, token=hugging_face_api_key)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    return tokenizer

def main(args):
    # config
    export_root, args = setup_train(args)
    tokenizer = load_tokenizer(args)
    wandb.init(project="prompt-tuning-law", name=f"{args.llm_name}_{args.dataset_name}", config=args.__dict__)
    # dataset
    ir_dataset = MSMARCODataset(args, tokenizer)
    train_dataset, test_dataset = ir_dataset.get_dataset()

    train_dataloader = DataLoader(train_dataset['train'], shuffle=True, collate_fn=default_data_collator, batch_size=args.batch_size, pin_memory=True)
    eval_dataloader = DataLoader(train_dataset['test'], collate_fn=default_data_collator, batch_size=args.batch_size, pin_memory=True)
    test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=args.batch_size, pin_memory=True)

    # creating model
    peft_config = PromptTuningConfig(
        task_type=TaskType.CAUSAL_LM,
        # num_virtual_tokens=args.num_virtual_tokens ÏÑ§Ï†ïÏùÑ Î≥¥Í≥†,
        # LLMÏùò ÏûÑÎ≤†Îî© Ï∞®ÏõêÍ≥º ÎèôÏùºÌïú ÌÅ¨Í∏∞Î•º Í∞ÄÏßÑ num_virtual_tokens Í∞úÏàòÎßåÌÅºÏùò ÏÉàÎ°úÏö¥ torch.nn.Parameter Í∞ùÏ≤¥
        # Ï¶â, Soft PromptÏùò ÌååÎùºÎØ∏ÌÑ∞Î•º ÏÉùÏÑ±Ìï®
        num_virtual_tokens=args.num_virtual_tokens,
        #prompt_tuning_init=PromptTuningInit.TEXTÏôÄ prompt_tuning_init_text=args.prompt_tuning_init_text ÏÑ§Ï†ïÏùÑ Î≥¥Í≥†,
        # args.prompt_tuning_init_textÏóê ÏßÄÏ†ïÎêú ÌÖçÏä§Ìä∏(Ïòà: "please generate query for document" Î∞òÎ≥µ)Î•º
        # LLMÏùò ÏõêÎûò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÏôÄ ÏûÑÎ≤†Îî© Î†àÏù¥Ïñ¥Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÏûÑÎ≤†Îî© Î≤°ÌÑ∞Î°ú Î≥ÄÌôò
        prompt_tuning_init=PromptTuningInit.TEXT,
        prompt_tuning_init_text=args.prompt_tuning_init_text,
        tokenizer_name_or_path=args.model_name_or_path,
    )
    #  modelÏùÄ LLMÏùò Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Î•º Ìè¨Ìï®(Í∏∞Ï°¥ ÏûÑÎ≤†Îî©, Î™®Îç∏ ÎÇ¥Î∂Ä ÌååÎùºÎØ∏ÌÑ∞ Îì±)
    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path,
                                                 token=hugging_face_api_key,
                                                 torch_dtype=torch.float16,
                                                 device_map="auto")

    # get_peft_modelÎ•º ÌÜµÌï¥, ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú Î°úÎìúÎêú modelÏùò ÌååÎùºÎØ∏ÌÑ∞ÏùÑ Ï≤òÎ¶¨Ìï®
    # LLMÏùò ÎåÄÎ∂ÄÎ∂ÑÏùò ÏõêÎûò ÌååÎùºÎØ∏ÌÑ∞ Ï†ÑÏ≤¥, Ï¶â ÏûÑÎ≤†Îî© Î∞è Î™®Îç∏ ÎÇ¥Î∂Ä ÌååÎùºÎØ∏ÌÑ∞Î•º requires_grad=FalseÎ°ú ÏÑ§Ï†ïÌïòÏó¨ freeze
    # peft_configÏóê Ï†ïÏùòÎêú ÏÉàÎ°úÏö¥ Prompt Tuning ÌååÎùºÎØ∏ÌÑ∞Ïù∏ Soft Prompt ÏûÑÎ≤†Îî© Î†àÏù¥Ïñ¥Ïùò ÌååÎùºÎØ∏ÌÑ∞)Îßå
    # requires_grad=TrueÎ°ú ÏÑ§Ï†ïÌïòÏó¨ ÌïôÏäµ Í∞ÄÎä•ÌïòÍ≤å Ìï®
    model = get_peft_model(model, peft_config)
    model.gradient_checkpointing_enable()
    # model.print_trainable_parameters()

    # optimizer and lr scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
    lr_scheduler = get_linear_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=(len(train_dataloader) * args.num_epochs),
    )

    # training and evaluation
    model = model.to(args.device)
    original_eval_loss = 999999
    early_stop_epoch = 0
    for epoch in range(args.num_epochs):
        if early_stop_epoch > 5:
            print('Terminating because of early stopping!')
            break
        total_train_loss = 0
        total_eval_loss = 0
        avg_train_loss = AverageMeter()
        avg_val_loss = AverageMeter()
        model.train()
        for step, batch in enumerate(tqdm(train_dataloader)):
            batch = {k: v.to(args.device) for k, v in batch.items()}
            batch = _sanitize_and_check(batch, model, tokenizer)
            with torch.no_grad():
                outputs = model(**batch)
                loss = outputs.loss
                total_train_loss += loss.detach().float()
                avg_train_loss.update(loss.detach().float().item())
                loss.backward()
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

        # evaluate eval dataset
        eval_preds = []
        for step, batch in enumerate(tqdm(eval_dataloader)):
            batch = {k: v.to(args.device) for k, v in batch.items()}
            batch = _sanitize_and_check(batch, model, tokenizer)
            with torch.no_grad():
                outputs = model(**batch)
            loss = outputs.loss
            total_eval_loss += loss.detach().float()
            avg_val_loss.update(loss.detach().float().item())
            eval_preds.extend(
                tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)
            )

        # get metrics
        train_epoch_loss = total_train_loss / len(train_dataloader)
        train_ppl = torch.exp(train_epoch_loss).detach().float().item()
        eval_epoch_loss = total_eval_loss / len(eval_dataloader)
        eval_ppl = torch.exp(eval_epoch_loss).detach().float().item()
        # saving model
        if avg_val_loss.avg < original_eval_loss:
            original_eval_loss = avg_val_loss.avg
            early_stop_epoch = 0
            filepath = Path(export_root).joinpath(args.peft_model_id)
            print(f'new best val loss, model saved at {filepath}')
            model.save_pretrained(filepath)
        else:
            early_stop_epoch += 1
        # logger
        wandb.log({
            'epoch': epoch,
            'train_loss': avg_train_loss.avg,
            'val_loss': avg_val_loss.avg,
            'train_ppl': train_ppl,
            'eval_ppl': eval_ppl
        })
        print(f"{epoch=}: {train_ppl=} {avg_train_loss.avg=} {eval_ppl=} {avg_val_loss.avg=}")

    print('-------- ÌèâÍ∞Ä --------')
    # evaluate test dataset
    total_test_loss = 0
    avg_test_loss = AverageMeter()
    model.eval()
    for step, batch in enumerate(tqdm(test_dataloader)):
        batch = {k: v.to(args.device) for k, v in batch.items()}
        batch = _sanitize_and_check(batch, model, tokenizer)
        with torch.no_grad():
            outputs = model(**batch)
        loss = outputs.loss
        total_test_loss += loss.detach().float()
        avg_test_loss.update(loss.detach().float().item())
        preds = torch.argmax(outputs.logits, dim=-1)

        labels = batch['labels'].clone().detach().cpu()
        labels[labels == -100] = tokenizer.pad_token_id

        decoded_preds = tokenizer.batch_decode(preds.detach().cpu(), skip_special_tokens=True)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

        for i in range(len(decoded_preds)):
            print(f"\n[ÏÉòÌîå {i + 1}]")
            print("üîπ Î™®Îç∏ ÏÉùÏÑ±:", decoded_preds[i])
            print("üî∏ Ï†ïÎãµ Î†àÏù¥Î∏î:", decoded_labels[i])
    test_epoch_loss = total_test_loss / len(test_dataloader)
    test_ppl = torch.exp(test_epoch_loss).detach().float().item()
    wandb.log({
        'test_loss': avg_test_loss.avg,
        'test_ppl': test_ppl
    })
    print(f"{epoch=}: {train_ppl=} {avg_train_loss.avg=} {eval_ppl=} {avg_val_loss.avg=} {test_ppl=} {avg_test_loss.avg=}")
    wandb.finish()

if __name__ == "__main__":
    base_args = PromptTuringArgs()
    parser = argparse.ArgumentParser()
    parser.add_argument("--num_virtual_tokens", type=int, help="num virtual tokens for prompt")
    parser.add_argument("--llm_name", type=str, help="model name")
    parser.add_argument("--device_idx", type=str, help="device id")
    parser.add_argument("--prompt_num", type=int, help="prompt number")
    parser.add_argument("--dataset_name", type=str, help="dataset name")
    parser.add_argument("--train_data", type=str, help="train data path")
    parser.add_argument("--eval_data", type=str, help="eval data path")
    parser.add_argument("--test_data", type=str, help="test data path")
    parser.add_argument("--few_shot_num", type=int, help="few shot setting")
    args = parser.parse_args(namespace=base_args)
    args = reset_args(args)
    main(args)
# ÏúÑÏπòÎäî ÏµúÏÉÅÎã® (SPTAR Î∞îÎ°ú ÏïÑÎûò)
# --train_data xuyang/data/law/prompt_tuning_50_train_text.csv --eval_data xuyang/data/msmarco_50/prompt_tuning_50_test_text.csv --test_data xuyang/data/msmarco_50/prompt_tuning_50_test_text.csv
# python -m xuyang.prompt_train_v1 --device_idx 1 --num_virtual_tokens 50 --prompt_num 3 --llm_name tiny_llama-1.1b --few_shot_num 50 --dataset_name law

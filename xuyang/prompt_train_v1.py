from transformers import AutoModelForCausalLM
from peft import get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType
import os
import wandb
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
from transformers import default_data_collator, get_linear_schedule_with_warmup
from tqdm import tqdm
from pathlib import Path
import argparse
from dotenv import load_dotenv
from xuyang.args import PromptTuringArgs
from xuyang.utils import reset_args, AverageMeter, setup_train
from xuyang.dataset import MSMARCODataset

load_dotenv()

hugging_face_api_key = os.getenv("HUGGINGFACE_API_KEY")

import torch

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

torch.cuda.empty_cache()
torch.cuda.ipc_collect()

def load_tokenizer(args):
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, token=hugging_face_api_key)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    return tokenizer

def main(args):
    # config
    export_root, args = setup_train(args)
    tokenizer = load_tokenizer(args)
    wandb.init(project="prompt-tuning-law", name=f"{args.llm_name}_{args.dataset_name}", config=args.__dict__)
    # dataset
    ir_dataset = MSMARCODataset(args, tokenizer)
    train_dataset, test_dataset = ir_dataset.get_dataset()

    train_dataloader = DataLoader(train_dataset['train'], shuffle=True, collate_fn=default_data_collator, batch_size=args.batch_size, pin_memory=True)
    eval_dataloader = DataLoader(train_dataset['test'], collate_fn=default_data_collator, batch_size=args.batch_size, pin_memory=True)
    test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=args.batch_size, pin_memory=True)

    # creating model
    peft_config = PromptTuningConfig(
        task_type=TaskType.CAUSAL_LM,
        # num_virtual_tokens=args.num_virtual_tokens ì„¤ì •ì„ ë³´ê³ ,
        # LLMì˜ ìž„ë² ë”© ì°¨ì›ê³¼ ë™ì¼í•œ í¬ê¸°ë¥¼ ê°€ì§„ num_virtual_tokens ê°œìˆ˜ë§Œí¼ì˜ ìƒˆë¡œìš´ torch.nn.Parameter ê°ì²´
        # ì¦‰, Soft Promptì˜ íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•¨
        num_virtual_tokens=args.num_virtual_tokens,
        #prompt_tuning_init=PromptTuningInit.TEXTì™€ prompt_tuning_init_text=args.prompt_tuning_init_text ì„¤ì •ì„ ë³´ê³ ,
        # args.prompt_tuning_init_textì— ì§€ì •ëœ í…ìŠ¤íŠ¸(ì˜ˆ: "please generate query for document" ë°˜ë³µ)ë¥¼
        # LLMì˜ ì›ëž˜ í† í¬ë‚˜ì´ì €ì™€ ìž„ë² ë”© ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ìž„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        prompt_tuning_init=PromptTuningInit.TEXT,
        prompt_tuning_init_text=args.prompt_tuning_init_text,
        tokenizer_name_or_path=args.model_name_or_path,
    )
    #  modelì€ LLMì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í¬í•¨(ê¸°ì¡´ ìž„ë² ë”©, ëª¨ë¸ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ë“±)
    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, token=hugging_face_api_key)

    # get_peft_modelë¥¼ í†µí•´, ë‚´ë¶€ì ìœ¼ë¡œ ë¡œë“œëœ modelì˜ íŒŒë¼ë¯¸í„°ì„ ì²˜ë¦¬í•¨
    # LLMì˜ ëŒ€ë¶€ë¶„ì˜ ì›ëž˜ íŒŒë¼ë¯¸í„° ì „ì²´, ì¦‰ ìž„ë² ë”© ë° ëª¨ë¸ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ë¥¼ requires_grad=Falseë¡œ ì„¤ì •í•˜ì—¬ freeze
    # peft_configì— ì •ì˜ëœ ìƒˆë¡œìš´ Prompt Tuning íŒŒë¼ë¯¸í„°ì¸ Soft Prompt ìž„ë² ë”© ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„°)ë§Œ
    # requires_grad=Trueë¡œ ì„¤ì •í•˜ì—¬ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ í•¨
    model = get_peft_model(model, peft_config)
    model.gradient_checkpointing_enable()
    # model.print_trainable_parameters()

    # optimizer and lr scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
    lr_scheduler = get_linear_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=(len(train_dataloader) * args.num_epochs),
    )

    # training and evaluation
    model = model.to(args.device)
    original_eval_loss = 999999
    early_stop_epoch = 0
    for epoch in range(args.num_epochs):
        if early_stop_epoch > 5:
            print('Terminating because of early stopping!')
            break
        total_train_loss = 0
        total_eval_loss = 0
        avg_train_loss = AverageMeter()
        avg_val_loss = AverageMeter()
        model.train()
        for step, batch in enumerate(tqdm(train_dataloader)):
            batch = {k: v.to(args.device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            total_train_loss += loss.detach().float()
            avg_train_loss.update(loss.detach().float().item())
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

        # evaluate eval dataset
        eval_preds = []
        for step, batch in enumerate(tqdm(eval_dataloader)):
            batch = {k: v.to(args.device) for k, v in batch.items()}
            with torch.no_grad():
                outputs = model(**batch)
            loss = outputs.loss
            total_eval_loss += loss.detach().float()
            avg_val_loss.update(loss.detach().float().item())
            eval_preds.extend(
                tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)
            )

        # get metrics
        train_epoch_loss = total_train_loss / len(train_dataloader)
        train_ppl = torch.exp(train_epoch_loss).detach().float().item()
        eval_epoch_loss = total_eval_loss / len(eval_dataloader)
        eval_ppl = torch.exp(eval_epoch_loss).detach().float().item()
        # saving model
        if avg_val_loss.avg < original_eval_loss:
            original_eval_loss = avg_val_loss.avg
            early_stop_epoch = 0
            filepath = Path(export_root).joinpath(args.peft_model_id)
            print(f'new best val loss, model saved at {filepath}')
            model.save_pretrained(filepath)
        else:
            early_stop_epoch += 1
        # logger
        wandb.log({
            'epoch': epoch,
            'train_loss': avg_train_loss.avg,
            'val_loss': avg_val_loss.avg,
            'train_ppl': train_ppl,
            'eval_ppl': eval_ppl
        })
        print(f"{epoch=}: {train_ppl=} {avg_train_loss.avg=} {eval_ppl=} {avg_val_loss.avg=}")

    print('-------- í‰ê°€ --------')
    # evaluate test dataset
    total_test_loss = 0
    avg_test_loss = AverageMeter()
    model.eval()
    for step, batch in enumerate(tqdm(test_dataloader)):
        batch = {k: v.to(args.device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
        loss = outputs.loss
        total_test_loss += loss.detach().float()
        avg_test_loss.update(loss.detach().float().item())
        preds = torch.argmax(outputs.logits, dim=-1)

        labels = batch['labels'].clone().detach().cpu()
        labels[labels == -100] = tokenizer.pad_token_id

        decoded_preds = tokenizer.batch_decode(preds.detach().cpu(), skip_special_tokens=True)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

        for i in range(len(decoded_preds)):
            print(f"\n[ìƒ˜í”Œ {i + 1}]")
            print("ðŸ”¹ ëª¨ë¸ ìƒì„±:", decoded_preds[i])
            print("ðŸ”¸ ì •ë‹µ ë ˆì´ë¸”:", decoded_labels[i])
    test_epoch_loss = total_test_loss / len(test_dataloader)
    test_ppl = torch.exp(test_epoch_loss).detach().float().item()
    wandb.log({
        'test_loss': avg_test_loss.avg,
        'test_ppl': test_ppl
    })
    print(f"{epoch=}: {train_ppl=} {avg_train_loss.avg=} {eval_ppl=} {avg_val_loss.avg=} {test_ppl=} {avg_test_loss.avg=}")
    wandb.finish()

if __name__ == "__main__":
    base_args = PromptTuringArgs()
    parser = argparse.ArgumentParser()
    parser.add_argument("--num_virtual_tokens", type=int, help="num virtual tokens for prompt")
    parser.add_argument("--llm_name", type=str, help="model name")
    parser.add_argument("--device_idx", type=str, help="device id")
    parser.add_argument("--prompt_num", type=int, help="prompt number")
    parser.add_argument("--dataset_name", type=str, help="dataset name")
    parser.add_argument("--train_data", type=str, help="train data path")
    parser.add_argument("--eval_data", type=str, help="eval data path")
    parser.add_argument("--test_data", type=str, help="test data path")
    parser.add_argument("--few_shot_num", type=int, help="few shot setting")
    args = parser.parse_args(namespace=base_args)
    args = reset_args(args)
    main(args)
# ìœ„ì¹˜ëŠ” ìµœìƒë‹¨ (SPTAR ë°”ë¡œ ì•„ëž˜)
# --train_data xuyang/data/law/prompt_tuning_50_train_text.csv --eval_data xuyang/data/msmarco_50/prompt_tuning_50_test_text.csv --test_data xuyang/data/msmarco_50/prompt_tuning_50_test_text.csv
# python -m xuyang.prompt_train_v1 --device_idx 1 --num_virtual_tokens 50 --prompt_num 3 --llm_name llama-3.2-1b --few_shot_num 50 --dataset_name law
